{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A training ground for sparse-coded autoencoders:\n",
    "\n",
    "**I am inheriting thise materials from an archived private repo.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import  DataLoader\n",
    "#\n",
    "import dataTools as D\n",
    "import tools as T\n",
    "from datetime import datetime\n",
    "import os\n",
    "#\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "%precision 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some global constants:\n",
    "num_epoch = 200\n",
    "batch_size = 100\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select your database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_name = 'CelebA'\n",
    "############################\n",
    "if database_name == 'CelebA':\n",
    "    batch_size = 100\n",
    "    root = '/ndata/CelebA/128_crop/'\n",
    "    img_names_list_train = './dataset_splits/CelebA/CelebA_train.txt'\n",
    "    img_names_list_valid = './dataset_splits/CelebA/CelebA_valid.txt'\n",
    "    img_size = (3, 128, 128)\n",
    "elif database_name == 'CYale':\n",
    "    root = '/ndata/ferdowsi/CYale/'\n",
    "    img_names_list_train = './dataset_splits/CYale/CYale_train.txt'\n",
    "    img_names_list_valid = './dataset_splits/CYale/CYale_valid.txt'   \n",
    "    img_size = (1, 168, 192)\n",
    "num_channel = img_size[0]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the database classes for train and valid splits:\n",
    "\n",
    "Make sure to adjust the ``num_workers``appropriately based on your data/GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = D.imgRead_fromList(root, img_names_list_train, img_size)\n",
    "dataset_valid = D.imgRead_fromList(root, img_names_list_valid, img_size)\n",
    "# Initialize the mini-batch dataloaders:\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "dataloader_valid = DataLoader(dataset_valid, batch_size=batch_size, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_blocks = 6\n",
    "num_filts = [40, 40, 40, 40, 40, 10]\n",
    "scale_factor = [1, 2, 1, 2, 1, 2]\n",
    "num_codes = 20\n",
    "neck_dim = 512\n",
    "k = 256\n",
    "############\n",
    "from models import Autoencoder\n",
    "net = Autoencoder(img_size, num_blocks, num_filts, scale_factor, num_codes, neck_dim, k).to(device)\n",
    "# print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the loss-function and optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(x_rec, x):\n",
    "    loss_BCE = nn.BCELoss()\n",
    "    #loss_MSE = nn.MSELoss()\n",
    "    #return loss_L1_Charbonnier\n",
    "    return loss_BCE(x_rec, x)\n",
    "#############################################\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01,weight_decay=0)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, 'min', verbose=True, factor=0.99, min_lr=0.000001,patience=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.train()\n",
    "for i_epoch in range(num_epoch):\n",
    "    loss_train = 0.0\n",
    "    print('---------------- epoch = ', i_epoch + 1, '/',num_epoch, ' ----------')\n",
    "    for i_batch, inp_train in enumerate(dataloader_train):\n",
    "\n",
    "        inp_train = inp_train['image'].to(device) \n",
    "        out_train, code_train = net(inp_train)\n",
    "        out_train.sigmoid_()  # Remeber to apply it also on valid-test sets. Or move it to the mode!\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_function(out_train, inp_train)\n",
    "        loss.backward()  \n",
    "        optimizer.step()\n",
    "        scheduler.step(loss)\n",
    "        loss_train += loss.item()\n",
    "        if i_epoch < 2:\n",
    "             print(f\"b({i_batch}): l = {loss.item():.3f}\", end=\" | \")   \n",
    "    print('Avg train loss = ', loss_train/len(dataloader_train))\n",
    "    with torch.no_grad():\n",
    "        loss_valid = 0.0\n",
    "        for _, inputs_valid in enumerate(dataloader_valid):\n",
    "            inp_valid = inp_valid['image'].to(device) \n",
    "            out_valid, code_valid = net(inp_valid)\n",
    "            out_valid.sigmoid_()\n",
    "            \n",
    "            loss_valid += loss_function(out_valid,inp_valid).item()\n",
    "        print('Avg validation loss = ', loss_valid/len(dataloader_valid))  \n",
    "\n",
    "# Note that average validation error for each epoch uses the most recent parameters, while\n",
    "# the average training error is taking all updates into account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To save the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To stamp with current time:\n",
    "now = str(datetime.timestamp(datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing an informative name for the model:\n",
    "net_name = database_name + \\\n",
    "            '_filts' + D.list2str(num_filts)+ \\\n",
    "            '_scale' + D.list2str(scale_factor) +\\\n",
    "            '_codes' + str(num_codes) +\\\n",
    "            '_dim' + str(neck_dim) +\\\n",
    "            '_k' + str(k) + \\\n",
    "            '.pth'\n",
    "#########################\n",
    "net_root = './weights'\n",
    "net_path = D.pathStamper(os.path.join(net_root, net_name), now)\n",
    "print(net_path)\n",
    "torch.save(net.state_dict(), net_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some basic visualization and evaluation of reconstruction performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "D.imShow(inp_valid, idx=idx)\n",
    "D.imShow(out_valid, idx=idx)\n",
    "\n",
    "print(torch.norm(inp_train - out_train).pow(2)/torch.norm(inp_train).pow(2))\n",
    "print(torch.norm(inp_valid - out_valid).pow(2)/torch.norm(inp_valid).pow(2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch1.4_cv_faiss]",
   "language": "python",
   "name": "conda-env-pytorch1.4_cv_faiss-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
