{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noise Ground:\n",
    "\n",
    "Having trained some networks, here we are trying to play around with the codes by adding some ambiguization noise and see their decoded outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import  DataLoader\n",
    "from torchvision import utils\n",
    "#\n",
    "import dataTools as D\n",
    "import tools as T\n",
    "#\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "from skimage.measure import compare_psnr,compare_ssim\n",
    "from cv2 import imwrite, imread, IMWRITE_JPEG_QUALITY, COLOR_BGR2RGB\n",
    "%precision 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some global constants:\n",
    "batch_size = 1000\n",
    "device = torch.device(\"cuda:1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select your database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_name = 'CelebA'\n",
    "############################\n",
    "if database_name == 'CelebA':\n",
    "    batch_size = 100\n",
    "    #root = '/path/to/CelebA/128_crop/'\n",
    "    root = '/path/to/CelebA/128_crop/'\n",
    "    img_names_list_test = './dataset_splits/CelebA/CelebA_test.txt'\n",
    "    img_size = (3, 128, 128)\n",
    "elif database_name == 'CYale':\n",
    "    root = '/path/to/CYale/'\n",
    "    img_names_list_test = './dataset_splits/CYale/CYale_test.txt'  \n",
    "    img_size = (1, 168, 192)\n",
    "num_channel = img_size[0]   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the database class for the test split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = D.imgRead_fromList(root, img_names_list_test, img_size)\n",
    "# Initialize the mini-batch dataloader:\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './weights/CelebA_filts40-40-40-40-40-10_scale1-2-1-2-1-2_codes20_dim512_k256_stmp1587121097.082492.pth'\n",
    "##########################################  Loading a trained model ##############################\n",
    "num_blocks = 6\n",
    "num_filts = [40, 40, 40, 40, 40, 10]\n",
    "scale_factor = [1, 2, 1, 2, 1, 2]\n",
    "num_codes = 20\n",
    "neck_dim = 512\n",
    "k = 128\n",
    "#######\n",
    "from models import Autoencoder\n",
    "net = Autoencoder(img_size, num_blocks, num_filts, scale_factor, num_codes, neck_dim, k)\n",
    "net.load_state_dict(torch.load(model_path))\n",
    "net.eval()\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the network on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we load only one mini-batch:\n",
    "with torch.no_grad():\n",
    "    for i, inp in enumerate(dataloader):\n",
    "        inp = inp['image'].to(device) \n",
    "        out, code = net(inp)\n",
    "        out.sigmoid_()\n",
    "        break\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some basic visualization and evaluation of the reconstruction performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 20 # To show\n",
    "ind_i = 10  # To save\n",
    "D.imShow(inp, idx=idx)\n",
    "D.imShow(out, idx=idx)\n",
    "\n",
    "print(torch.norm(inp - out).pow(2)/torch.norm(inp).pow(2))\n",
    "utils.save_image(inp[ind_i:ind_i + 8,:,:,:], 'samples/inputs.png', nrow=8)\n",
    "utils.save_image(out[ind_i:ind_i + 8,:,:,:], 'samples/reconstructed.png', nrow=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To see the effect of sparsity on the reconstruction performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoding with less sparsity:\n",
    "code_prime = torch.clone(code).cpu().detach()\n",
    "code_prime = T.KBest(code_prime, 64)\n",
    "out_prime = net.decoder(code_prime.to(device)).sigmoid_()\n",
    "D.imShow(out_prime, idx=idx)\n",
    "utils.save_image(out_prime[ind_i:ind_i + 8,:,:,:], 'samples/lessSparsity.png', nrow=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To see the characteristics of code-maps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting some of the code-maps to zero:\n",
    "code_prime = torch.clone(code).cpu().detach()\n",
    "i_s = 0\n",
    "i_e = 12\n",
    "code_prime[:,i_s:i_e,:] = 0\n",
    "out_prime = net.decoder(code_prime.to(device)).sigmoid_()\n",
    "D.imShow(out_prime, idx=idx)\n",
    "utils.save_image(out_prime[ind_i:ind_i + 8,:,:,:], 'samples/zeroed.png', nrow=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting some other code-maps to zero:\n",
    "code_prime = torch.clone(code).cpu().detach()\n",
    "i_s = 12\n",
    "i_e = 20\n",
    "code_prime[:,i_s:i_e,:] = 0\n",
    "out_prime = net.decoder(code_prime.to(device)).sigmoid_()\n",
    "D.imShow(out_prime, idx=idx)\n",
    "utils.save_image(out_prime[ind_i:ind_i + 8,:,:,:], 'samples/zeroed2.png', nrow=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ambiguation noise:\n",
    "\n",
    "As the main application of the paper, let's add some ambiguating noise to the zero coefficients of the code-maps. The idea is to add minimal and undistinguishable noise, while maximally destroying the content of the resulting decoded image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ambiguation noise on the complement of support:\n",
    "# Note that the number of added noise values is k_prime - k.\n",
    "k_prime = 2*k\n",
    "code_tilde = torch.clone(code).cpu().detach()\n",
    "code_tilde = T.ambiguate(code_tilde, k_prime=k_prime)\n",
    "out_tilde = net.decoder(code_tilde.to(device)).sigmoid_()\n",
    "D.imShow(out_tilde, idx=idx)\n",
    "utils.save_image(out_tilde[ind_i:ind_i + 8,:,:,:], 'samples/ambiguated.png', nrow=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attacks:\n",
    "\n",
    "As a very basic attack, here we randomly put $k$ out of $k'$ non-zero values to zero. Without any extra knowledge, the adversary has to make $k' \\choose k$ such guesses to reconstruct the original image.\n",
    "\n",
    "Can this system be attacked in other more intricate ways? In future works, we will study different attacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly selecting k out of k':\n",
    "code_hat = torch.clone(code_tilde).cpu().detach()\n",
    "code_hat = T.random_guess(code_hat, k)    \n",
    "out_hat = net.decoder(code_hat.to(device)).sigmoid_()\n",
    "D.imShow(out_hat, idx=idx)\n",
    "utils.save_image(out_hat[ind_i:ind_i + 8,:,:,:], 'samples/disambiguated.png', nrow=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quantitative quality assessment:\n",
    "\n",
    "* PSNR\n",
    "* SSIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psnr_outputs = []\n",
    "ssim_outputs = []\n",
    "\n",
    "psnr_tildes = []\n",
    "ssim_tildes = []\n",
    "\n",
    "psnr_hats = []\n",
    "ssim_hats = []\n",
    "\n",
    "for i in range(inp.shape[0]):\n",
    "    psnr_outputs.append(\n",
    "        compare_psnr(\n",
    "        inp[i,:,:,:].squeeze(0).transpose(0,2).cpu().detach().numpy(),\n",
    "        out[i,:,:,:].squeeze(0).transpose(0,2).cpu().detach().numpy()) )\n",
    "    \n",
    "    ssim_outputs.append(\n",
    "        compare_ssim(\n",
    "        inp[i,:,:,:].squeeze(0).transpose(0,2).cpu().detach().numpy(),\n",
    "        out[i,:,:,:].squeeze(0).transpose(0,2).cpu().detach().numpy(), multichannel=True) )\n",
    "    \n",
    "    \n",
    "    \n",
    "    psnr_tildes.append(\n",
    "        compare_psnr(\n",
    "        inp[i,:,:,:].squeeze(0).transpose(0,2).cpu().detach().numpy(),\n",
    "        out_tilde[i,:,:,:].squeeze(0).transpose(0,2).cpu().detach().numpy()) )\n",
    "    \n",
    "    ssim_tildes.append(\n",
    "        compare_ssim(\n",
    "        inp[i,:,:,:].squeeze(0).transpose(0,2).cpu().detach().numpy(),\n",
    "        out_tilde[i,:,:,:].squeeze(0).transpose(0,2).cpu().detach().numpy(), multichannel=True) )\n",
    "    \n",
    "    \n",
    "    psnr_hats.append(\n",
    "        compare_psnr(\n",
    "        inp[i,:,:,:].squeeze(0).transpose(0,2).cpu().detach().numpy(),\n",
    "        out_hat[i,:,:,:].squeeze(0).transpose(0,2).cpu().detach().numpy()) )\n",
    "    \n",
    "    ssim_hats.append(\n",
    "        compare_ssim(\n",
    "        inp[i,:,:,:].squeeze(0).transpose(0,2).cpu().detach().numpy(),\n",
    "        out_hat[i,:,:,:].squeeze(0).transpose(0,2).cpu().detach().numpy(), multichannel=True) )\n",
    "    \n",
    "    \n",
    "############################    \n",
    "print(np.mean(psnr_outputs))\n",
    "print(np.mean(ssim_outputs))\n",
    "\n",
    "\n",
    "print(np.mean(psnr_tildes))\n",
    "print(np.mean(ssim_tildes))\n",
    "\n",
    "print(np.mean(psnr_hats))\n",
    "print(np.mean(ssim_hats))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psnr_jpg = []\n",
    "ssim_jpg = []\n",
    "for i in range(inp.shape[0]):\n",
    "    \n",
    "    img = inp[i,:,:,:].squeeze(0).transpose(0,2).cpu().numpy() \n",
    "    imwrite('tmp.jpg', img * 256, [IMWRITE_JPEG_QUALITY, 4])\n",
    "    image = imread('tmp.jpg' , COLOR_BGR2RGB)\n",
    "    image = image.astype('float32')/256\n",
    "    psnr_jpg.append(compare_psnr(img, image))\n",
    "    ssim_jpg.append(compare_ssim(img, image, multichannel=True))\n",
    "\n",
    "    \n",
    "print(np.mean(psnr_jpg))\n",
    "print(np.mean(ssim_jpg))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## How many KBytes per compressed image:\n",
    "print(T.calculate_KBytes(512, 128, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The ratio of the compressed image to the key:\n",
    "print(T.calculate_KBytes(512, 128, 20) / T.calculate_KBytes(k_prime, 128, 20) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch1.4_cv_faiss]",
   "language": "python",
   "name": "conda-env-pytorch1.4_cv_faiss-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
